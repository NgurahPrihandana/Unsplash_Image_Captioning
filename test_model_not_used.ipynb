{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prihandana/Developer/Deep Learning/Unsplash_Image_Scrapper/Unsplash_Image_Captioning/Scrapping/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import ViTModel, ViTFeatureExtractor\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_path):\n",
    "    with open(tokenizer_path, 'r') as f:\n",
    "        tokenizer_data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(json.dumps(tokenizer_data))  # Convert dict to JSON string\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained ViT model and feature extractor\n",
    "def load_vit_model_and_processor():\n",
    "    model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "    model.eval() \n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "    return model, feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image using ViT\n",
    "def preprocess_image(image_path, feature_extractor):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    return inputs[\"pixel_values\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using ViT\n",
    "def extract_features(image_path, vit_model, feature_extractor):\n",
    "    pixel_values = preprocess_image(image_path, feature_extractor)\n",
    "    with torch.no_grad():\n",
    "        outputs = vit_model(pixel_values)\n",
    "        features = outputs.last_hidden_state.mean(dim=1).numpy()  # Global average pooling\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, tokenizer, image_features, max_len):\n",
    "    # Initialize the caption with the <start> token\n",
    "    caption = \"<start>\"\n",
    "    \n",
    "    # Generate caption one word at a time\n",
    "    for _ in range(max_len - 1):  # Generate up to max_len - 1 tokens\n",
    "        # Convert the current caption to a sequence\n",
    "        sequence = tokenizer.texts_to_sequences([caption])[0]\n",
    "        \n",
    "        # Pad the sequence to max_len - 1\n",
    "        sequence = pad_sequences([sequence], maxlen=max_len - 1, padding='post')\n",
    "        \n",
    "        # Predict the next word\n",
    "        predictions = model.predict([image_features, sequence], verbose=0)\n",
    "        \n",
    "        # Get the index of the word with the highest probability\n",
    "        next_word_idx = np.argmax(predictions[0, -1, :])\n",
    "        next_word = tokenizer.index_word.get(next_word_idx, \"<unk>\")\n",
    "        \n",
    "        # Stop if the <end> token is generated\n",
    "        if next_word == \"<end>\":\n",
    "            break\n",
    "        \n",
    "        # Append the predicted word to the caption\n",
    "        caption += ' ' + next_word\n",
    "    \n",
    "    # Remove the <start> token and return the generated caption\n",
    "    return caption.replace(\"<start>\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to generate a caption for an image\n",
    "def caption_image(image_path, model_path, tokenizer_path, max_len):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = load_tokenizer(tokenizer_path)\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    # Load the ViT model and feature extractor\n",
    "    vit_model, feature_extractor = load_vit_model_and_processor()\n",
    "    \n",
    "    # Extract image features\n",
    "    image_features = extract_features(image_path, vit_model, feature_extractor)\n",
    "    \n",
    "    # Generate the caption\n",
    "    caption = generate_caption(model, tokenizer, image_features, max_len)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: trees trees trees trees trees during photography end photography photo end end end photo photo end <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "image_path = \"test/test.jpg\"  # Path to the image\n",
    "model_path = \"models/Vit_LSTM_6.h5\"  # Path to the trained model\n",
    "tokenizer_path = \"tokenizer/tokenizer_6.json\"  \n",
    "max_len = 20\n",
    "caption = caption_image(image_path, model_path, tokenizer_path, max_len)\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
